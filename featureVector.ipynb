{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import KMeansClustering as kmc\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: sun_dbizycsfucqlktnk.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dbizycsfucqlktnk_features.npy\n",
      "Processing image: sun_dbsadqzxemdqaqih.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dbsadqzxemdqaqih_features.npy\n",
      "Processing image: sun_dcyykqocjeammfed.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dcyykqocjeammfed_features.npy\n",
      "Processing image: sun_dewwrshgfvqarkpm.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dewwrshgfvqarkpm_features.npy\n",
      "Processing image: sun_dezyrtacbxgoehoi.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dezyrtacbxgoehoi_features.npy\n",
      "Processing image: sun_dfvnumnuiqwvnexc.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dfvnumnuiqwvnexc_features.npy\n",
      "Processing image: sun_dgavxowprqmnpbtq.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dgavxowprqmnpbtq_features.npy\n",
      "Processing image: sun_dgpmwtllspwrcjtt.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dgpmwtllspwrcjtt_features.npy\n",
      "Processing image: sun_dhgkvwtvjrrztnja.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dhgkvwtvjrrztnja_features.npy\n",
      "Processing image: sun_digogqszkbfeatdg.jpg\n",
      "Features saved to featureVector/music_store/test/sun_digogqszkbfeatdg_features.npy\n",
      "Processing image: sun_djcmyzarzpihmmtp.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djcmyzarzpihmmtp_features.npy\n",
      "Processing image: sun_djfnuzjabouwewtz.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djfnuzjabouwewtz_features.npy\n",
      "Processing image: sun_djjanyrxxvqqixzv.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djjanyrxxvqqixzv_features.npy\n",
      "Processing image: sun_djskmwiztkajspnj.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djskmwiztkajspnj_features.npy\n",
      "Processing image: sun_djyusglcuskwollt.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djyusglcuskwollt_features.npy\n",
      "Processing image: sun_djzqfjivakzhjgpr.jpg\n",
      "Features saved to featureVector/music_store/test/sun_djzqfjivakzhjgpr_features.npy\n",
      "Processing image: sun_dkckcmhkciosnqin.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dkckcmhkciosnqin_features.npy\n",
      "Processing image: sun_dkowxswtzyxhfppk.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dkowxswtzyxhfppk_features.npy\n",
      "Processing image: sun_dlhohleroujqhdoh.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dlhohleroujqhdoh_features.npy\n",
      "Processing image: sun_dltolffubmjreost.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dltolffubmjreost_features.npy\n",
      "Processing image: sun_dlvwxrkkzbcoxyji.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dlvwxrkkzbcoxyji_features.npy\n",
      "Processing image: sun_dlweeybvbggfglms.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dlweeybvbggfglms_features.npy\n",
      "Processing image: sun_dmgwlvewospjbakm.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dmgwlvewospjbakm_features.npy\n",
      "Processing image: sun_dmhqafyuedxtmfpi.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dmhqafyuedxtmfpi_features.npy\n",
      "Processing image: sun_dmtppfpizgirzjrp.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dmtppfpizgirzjrp_features.npy\n",
      "Processing image: sun_dnivcoyxdjmpwpft.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dnivcoyxdjmpwpft_features.npy\n",
      "Processing image: sun_dnjetzwzvgscolki.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dnjetzwzvgscolki_features.npy\n",
      "Processing image: sun_dotbqoprlekalxxz.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dotbqoprlekalxxz_features.npy\n",
      "Processing image: sun_doyffalaqcbvjmzn.jpg\n",
      "Features saved to featureVector/music_store/test/sun_doyffalaqcbvjmzn_features.npy\n",
      "Processing image: sun_dqdzxmjduqvpggld.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dqdzxmjduqvpggld_features.npy\n",
      "Processing image: sun_dqfcxgcslhjrbjpo.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dqfcxgcslhjrbjpo_features.npy\n",
      "Processing image: sun_dqhlrqqfunkrtsqm.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dqhlrqqfunkrtsqm_features.npy\n",
      "Processing image: sun_dqhzroprooptodfi.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dqhzroprooptodfi_features.npy\n",
      "Processing image: sun_drpdilfdffgmhagi.jpg\n",
      "Features saved to featureVector/music_store/test/sun_drpdilfdffgmhagi_features.npy\n",
      "Processing image: sun_dsqsjtzzjcmtcczd.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dsqsjtzzjcmtcczd_features.npy\n",
      "Processing image: sun_dsvwpkmyttqgauxd.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dsvwpkmyttqgauxd_features.npy\n",
      "Processing image: sun_dtnzywaisgvniooe.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dtnzywaisgvniooe_features.npy\n",
      "Processing image: sun_dtodcfuukzbtgjpn.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dtodcfuukzbtgjpn_features.npy\n",
      "Processing image: sun_dugnxmnxvrdjfjkt.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dugnxmnxvrdjfjkt_features.npy\n",
      "Processing image: sun_dujlelovdemlcjnz.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dujlelovdemlcjnz_features.npy\n",
      "Processing image: sun_dvomgghmajnqcsoc.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dvomgghmajnqcsoc_features.npy\n",
      "Processing image: sun_dvtuasxqahokocut.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dvtuasxqahokocut_features.npy\n",
      "Processing image: sun_dvwhogjgjodujqga.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dvwhogjgjodujqga_features.npy\n",
      "Processing image: sun_dwxcvlsyflrcuuzl.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dwxcvlsyflrcuuzl_features.npy\n",
      "Processing image: sun_dxdaogypkfhopcmo.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dxdaogypkfhopcmo_features.npy\n",
      "Processing image: sun_dyyndxfawaumfzsa.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dyyndxfawaumfzsa_features.npy\n",
      "Processing image: sun_dziqbwdeukaunwcb.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dziqbwdeukaunwcb_features.npy\n",
      "Processing image: sun_dzkrnrhszuyvfyem.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dzkrnrhszuyvfyem_features.npy\n",
      "Processing image: sun_dzuamfdvfbbjouau.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dzuamfdvfbbjouau_features.npy\n",
      "Processing image: sun_dzyevsmvhawembyw.jpg\n",
      "Features saved to featureVector/music_store/test/sun_dzyevsmvhawembyw_features.npy\n"
     ]
    }
   ],
   "source": [
    "def extract_color_histograms(image_path, patch_size=32, bins=8):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at {image_path} could not be loaded. Check the path and file format.\")\n",
    "    \n",
    "    # Get the image dimensions\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    patches = []\n",
    "\n",
    "    # Loop over the image with the specified patch size\n",
    "    for i in range(0, height, patch_size):\n",
    "        for j in range(0, width, patch_size):\n",
    "            # Extract the patch (ensure it doesn't go out of bounds)\n",
    "            patch = image[i:min(i + patch_size, height), j:min(j + patch_size, width)]\n",
    "            \n",
    "            histograms = []\n",
    "\n",
    "            # Calculate histograms for each channel\n",
    "            for channel in range(3):  # Assuming BGR channels\n",
    "                hist = cv2.calcHist([patch], [channel], None, [bins], [0, 256])\n",
    "                hist = hist / np.sum(hist)  # Normalize the histogram\n",
    "                histograms.append(hist.flatten())\n",
    "            \n",
    "            # Concatenate histograms to create a feature vector\n",
    "            feature_vector = np.concatenate(histograms)\n",
    "            patches.append(feature_vector)\n",
    "    \n",
    "    return np.array(patches)\n",
    "def process_image_directory(image_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Loop through all files in the image directory\n",
    "    for image_file in os.listdir(image_directory):\n",
    "        image_path = os.path.join(image_directory, image_file)\n",
    "        \n",
    "        # Check if the current file is a valid file (not a directory)\n",
    "        if os.path.isfile(image_path):\n",
    "            print(f\"Processing image: {image_file}\")\n",
    "            \n",
    "            # Extract feature vectors (assuming the function is defined elsewhere)\n",
    "            feature_vectors = extract_color_histograms(image_path)\n",
    "            \n",
    "            # Define the output file path directly in the output directory\n",
    "            class_name = os.path.splitext(image_file)[0]  # Get the file name without extension\n",
    "            output_file = os.path.join(output_directory, f\"{class_name}_features.npy\")\n",
    "            \n",
    "            # Save the feature vectors to the output file\n",
    "            np.save(output_file, feature_vectors)\n",
    "            print(f\"Features saved to {output_file}\")\n",
    "\n",
    "# Specify the input and output directories\n",
    "image_directory = 'group01_2/group01/test/music_store'  # Path to the folder containing images\n",
    "output_directory = 'featureVector/music_store/test'           # Path to save the extracted feature vectors\n",
    "\n",
    "# Call the function\n",
    "process_image_directory(image_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_feature_vectors(directory):\n",
    "    all_features = []\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        # Check if the file ends with \".npy\" and load its contents\n",
    "        if os.path.isfile(file_path) and file.endswith(\".npy\"):\n",
    "            features = np.load(file_path)\n",
    "            all_features.append(features)\n",
    "    \n",
    "    # Combine all features into a single array\n",
    "    if all_features:\n",
    "        return np.vstack(all_features)\n",
    "    else:\n",
    "        return np.array([])  # Return an empty array if no features are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c1Train=load_all_feature_vectors('featureVector/bayou/train')\n",
    "c2Train=load_all_feature_vectors('featureVector/desert_vegetation/train')\n",
    "c3Train=load_all_feature_vectors('featureVector/music_store/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c1Test=load_all_feature_vectors('featureVector/bayou/test')\n",
    "c2Test=load_all_feature_vectors('featureVector/desert_vegetation/test')\n",
    "c3Test=load_all_feature_vectors('featureVector/music_store/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_features(all_features, n_clusters=32,mod=False):\n",
    "    centroid,idx=kmc.KMeansClustering(all_features,n_clusters,maxIteration=100,mod=mod)\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_bovw_representation(image_features,n_clusters=32,mod=False):\n",
    "    # Assign each feature vector to a cluster\n",
    "    centroid,idx=kmc.KMeansClustering(image_features,n_clusters,maxIteration=100,mod=mod)\n",
    "    # Count occurrences of each clusterd\n",
    "    cluster_counts = np.bincount(idx, minlength=n_clusters)\n",
    "    # Normalize the counts\n",
    "    bovw_vector = cluster_counts / len(image_features)\n",
    "    return bovw_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_for_bovw(input_directory, output_directory, n_clusters=32):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Iterate through all files in the input directory\n",
    "    for file in tqdm(os.listdir(input_directory)):\n",
    "        file_path = os.path.join(input_directory, file)\n",
    "        \n",
    "        # Check if the file is a valid .npy file\n",
    "        if file.endswith(\".npy\"):\n",
    "            # Load the image features\n",
    "            image_features = np.load(file_path)\n",
    "            \n",
    "            # Compute BoVW representation\n",
    "            bovw_vector = compute_bovw_representation(image_features, n_clusters)\n",
    "            \n",
    "            # Save the BoVW vector in the output directory\n",
    "            output_file = os.path.join(output_directory, file.replace(\"_features.npy\", \"_bovw.npy\"))\n",
    "            np.save(output_file, bovw_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all feature vectors...\n",
      "Clustering feature vectors into 32 clusters...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading all feature vectors...\")\n",
    "all_features = load_all_feature_vectors('featureVector/bayou/train')\n",
    "\n",
    "# Step 2: Perform K-means clustering\n",
    "print(\"Clustering feature vectors into 32 clusters...\")\n",
    "kmeans = cluster_features(all_features)\n",
    "\n",
    "# Step 3: Compute BoVW representation for all images\n",
    "print(\"Processing images to compute BoVW representations...\")\n",
    "process_images_for_bovw('featureVector/music/train','output/music')\n",
    "\n",
    "print(\"BoVW representations computed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering feature of class1\n",
      "Processing image: sun_aarhbeqcuzhoshba_features.npy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image at featureVector/bayou/train/sun_aarhbeqcuzhoshba_features.npy could not be loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustering feature of class1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m kmeansc1Train\u001b[38;5;241m=\u001b[39mcluster_features(c1Train)\n\u001b[0;32m----> 3\u001b[0m process_image_directory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatureVector/bayou/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/bayou/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mprocess_image_directory\u001b[0;34m(image_directory, output_directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Extract feature vectors (assuming the function is defined elsewhere)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m feature_vectors \u001b[38;5;241m=\u001b[39m extract_color_histograms(image_path)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Define the output file path directly in the output directory\u001b[39;00m\n\u001b[1;32m     42\u001b[0m class_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_file)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get the file name without extension\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mextract_color_histograms\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      7\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Image at featureVector/bayou/train/sun_aarhbeqcuzhoshba_features.npy could not be loaded."
     ]
    }
   ],
   "source": [
    "print('clustering feature of class1')\n",
    "kmeansc1Train=cluster_features(c1Train)\n",
    "process_image_directory('featureVector/bayou/train','output/bayou/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('clustering feature of class2')\n",
    "kmeansc1Train=cluster_features(c1Train)\n",
    "process_image_directory('featureVector/bayou/train','output/bayou/train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
